% \section{Event-driven Burst Buffer Simulator}
% \label{Sec:Simulation}

% We develop an event-driven simulator for burst buffer enabled HPC system, named \textbf{BBSim},
% from scratch in Python to mimic Cerberus scheduling Trinity.\NOTE{Fig has no info about Trinity}
% It roughly consists of 1,400 lines of code.
% Figure~\ref{Fig:JobFSM} illustrates the simulation lifetime of a job in BBSim
% using extended finite state machine (EFSM).
% The \textit{state} of the job changes depending on current status and
% happening \textit{events}, along with which an \textit{action} is taken.
% For example, at the very beginning, submitted $job_i$ will enter \textbf{Waiting Stage-in} state;
% at the meanwhile, $job_i$ is enqueued to $Q_I$ and scheduler is triggered to do scheduling.

\section{BBSim: An Event Driven Simulator of Cerberus}
\label{Sec:Simulation}

We develop an event-driven simulator, named \textbf{BBSim},
to simulate how Cerberus schedules jobs on burst-buffer-enabled HPC systems.
Apart from demonstrating the lifetime of a job scheduled by Cerberus,
Figure~\ref{Fig:JobFSM} also illustrates the various events in BBSim and how to handle
them in an event-driven-simulation approach.
The transitions of job \textit{state}, along with which an \textit{action} is triggered,
depend on the current state and the \textit{event} happened.
For example, when $job_i$ is submitted to the system,
it enters the \textit{Waiting Stage-in} state,
waiting to be scheduled in the job queue $Q_I$. 
The scheduler checks $job_i$'s resource demand and
dispatches it to run when sufficient resources are available.

% Whenever job enters one of its 3 phases, system resources are allocated:
% \begin{itemize}
%         \item $BB_{in}$ TB amount of burst buffer are allocated upon entering stage-in phase.
%         \item $CN$ number of compute nodes and $BB_{run}$ amount of burst buffer
%                 are allocated upon entering running phase.
%         \item $BB_{out}$ TB of burst buffer are allocated upon entering stage-out phase.
% \end{itemize}
% Various \texttt{release()} actions are of importance because, in addition to submission,
% \texttt{schedule()} is invoked to schedule waiting jobs
% whenever system resources are released:
% \begin{itemize}
%         \item When job's data is loaded from burst buffer to memory,
%                 burst buffer allocated at stage-in phase are released.
%         \item When job finishes running, system reclaims burst buffer nodes
%                 used for checkpointing.
%         \item When job's data is written to burst buffer from memory,
%                 compute nodes taken by job are returned.
%         \item When job's data is staged out to external disk,
%                 its burst buffer nodes are released.
% \end{itemize}
% 
% \begin{figure}[!t]
% \centering
%         \includegraphics[width=3.6in]{3PhaseJobFSM}
%         \caption{Finite State Machine of Scheduling Simulation}
% \label{Fig:JobFSM}
% \end{figure}


%Whenever $job_i$ enters one of the three phases, the system resources are allocated in the following way:
%\begin{itemize}
        %\item $BB_{in}$ TB amount of burst buffer are allocated upon entering the stage-in phase.
        %\item $CN$ number of compute nodes and $BB_{run}$ amount of burst buffer
                %are allocated upon entering the running phase.
        %\item $BB_{out}$ TB of burst buffer is allocated upon entering the stage-out phase.
%\end{itemize}

Various \texttt{release()} actions are important because, in addition to job submissions,
\texttt{schedule()} will be invoked to schedule the waiting jobs.
The allocated resource can only be released when one phase is finished.
Therefore, any \textit{dispatch} event, generated by the \texttt{schedule()} action,
follows a certain \textit{finish} event.
The allocated resources are released at the following time points:
\begin{itemize}
        \item When $job_i$ pre-fetches data from the burst buffer to the memory,
                the burst buffer allocated in the stage-in phase ($BB_{in}$) is released.
        \item When $job_i$ completes the computation,
                the system reclaims the burst buffer allocated in the running phase ($BB_{run}$).
        \item When $job_i$ output data is written to the burst buffer from the memory,
              compute nodes ($CN$) allocated in the running phase are released.
        \item When output data are drained out to the external storage,
                the burst buffer allocated in the stage-out phase ($BB_{out}$) is released.
                The simulation for $job_i$ completes.
\end{itemize}




% Notice that resource can only be released when certain phase is finished.
% Therefore, any \textit{dispatch} event, caused by \texttt{schedule()} action, actually happens
% at the meanwhile of a certain \textit{finish} event.
% The timestamps of all possible \textit{finish} events are calculated in the following way:
% \begin{align}
%         & TS_{f\_stagein} = TS_{s\_stagein} + \frac{bb\_in}{BW_{io\_to\_bb}}\label{Equ:FinIn} \\
%         & TS_{f\_loadin} = TS_{s\_run} + \frac{bb\_in}{BW_{bb\_to\_cn}}\label{Equ:FinMemIn} \\
%         & TS_{f\_run} = TS_{f\_loadin} + \frac{bb\_run}{BW_{cn\_to\_bb}} + rt\label{Equ:FinRun} \\
%         & TS_{f\_loadout} = TS_{s\_stageout} + \frac{bb\_out}{BW_{cn\_to\_bb}}\label{Equ:FinMemOut} \\
%         & TS_{f\_stageout} = TS_{f\_loadout} + \frac{bb\_out}{BW_{bb\_to\_io}} \label{Equ:FinOut}
% \end{align}
% where $TS_{f\_x}$ stands for the timestamps of finishing phase $x$,
% $TS_{s\_x}$ stands for the timestamps of starting phase $x$,
% $BW_{x\_to\_y}$ stands for the bandwidth between $x$ and $y$.
% 
% Though target on burst buffer system, BBSim also supports simulating non-BB HPC system.
% Besides, it is not coupled with Cerberus but easy to simulating many other scheduler.
% Both can be proved by the following experiments for various schedulers.


%The timestamps of all the \textit{finish} events are calculated as follows:
%\begin{align}
%        & TS_{f\_stagein} = TS_{s\_stagein} + \frac{bb\_in}{BW_{io\_to\_bb}}\label{Equ:FinIn} \\
%        & TS_{f\_loadin} = TS_{s\_run} + \frac{bb\_in}{BW_{bb\_to\_cn}}\label{Equ:FinMemIn} \\
%        & TS_{f\_run} = TS_{f\_loadin} + \frac{bb\_run}{BW_{cn\_to\_bb}} + rt\label{Equ:FinRun} \\
%        & TS_{f\_loadout} = TS_{s\_stageout} + \frac{bb\_out}{BW_{cn\_to\_bb}}\label{Equ:FinMemOut} \\
%        & TS_{f\_stageout} = TS_{f\_loadout} + \frac{bb\_out}{BW_{bb\_to\_io}} \label{Equ:FinOut}
%\end{align}
%where $TS_{f\_x}$ stands for the timestamps of the finishing phase $x$,
%$TS_{s\_x}$ stands for the timestamps of the starting phase $x$,
%$BW_{x\_to\_y}$ stands for the bandwidth between $x$ and $y$.
%BBSim does not simulate the data transfer between I/O nodes and PFS through the network.

% % system
% We consider simulating the full Trinity super computer\cite{TrinitySystem}.
% The number of compute nodes on Trinity is about 18,936,
% e.g. 9,436 Intel Haswell nodes
% and at least 9500 Intel Xeon Phi nodes.
% There are 16 cores on each processor, thus totally 302,976 cores.
% In the following experiments, we compare two identical system except that
% I/O nodes are replaced by the same number of burst buffer nodes.
% Eventually Trinity plans to delivery up to 576 burst buffer nodes of 3.7 PB,
% consisted of Trinity I/O nodes with PCIe SSD card.
% They are globally accessible as intermediate storage and distributed among cabinets.
% Sequential read/write speed of burst buffer nodes is 8.0 GBps.
% Bandwidth between CPU node and non-burst-buffer I/O node is set to 2.5 GBps.


Our Python implementation of BBSim includes three major modules.
The \texttt{simulator} module drives the event-driven simulation process.
Various job models, job demands, and schedulers are in \texttt{scheduler} module.
The \texttt{dp\_solver} module provides dynamic programming solutions for
the \texttt{Cerberus} scheduler in \texttt{scheduler}.
Though targeting on burst buffer enabled systems,
BBSim also supports simulating job scheduling on HPC systems without burst buffer.
Besides, it is easy to integrate other scheduling policies into BBSim.
The codebase of BBSim and Cerberus will be made public to the community.


We use BBSim to simulate
a burst-buffer-enabled HPC system shown in Figure~\ref{Fig:BBArchitecture}.
The system adopts traditional IBM Blue Gene/P architecture but burst buffer are
assumably available in the I/O nodes.
We simulate a system consists of 18,936 compute nodes,
e.g., 9,436 Intel Haswell nodes and 9,500 Intel Xeon Phi nodes.
The parameters related to burst buffer are taken from the 
NERSC-8 Use Case Scenarios technical report\cite{BBUseCase} on Trinity system\cite{TrinitySystem},
the next generation supercomputer in Los Alamos National Laboratory.
Specifically, there are 576 burst buffer enabled nodes with aggregated 3.7 PB storage,
which are globally accessible as the intermediate storage.
The sequential read/write speed of burst buffer nodes is 8.0 GBps.
The bandwidth between a CPU node and an I/O node is set to 2.5 GBps.
The time spent for job's I/O operation in our simulation is calculated by
dividing the amount of transferred data by the corresponding bandwidth.

\begin{figure}[htp]
        \centering
        \includegraphics[width=3.5in]{BBArchitecturewithBandwidth}
        \caption{A burst buffer enabled system simulated in BBSim. The bandwidth between compute nodes and burst buffer node is 8.0 GBps (denoted by red arrow). The bandwidth between compute nodes and I/O nodes, and the bandwidth between I/O layer and the outside InfiniBand Network are set to 2.5 GBps (denoted by purple arrow).}
\label{Fig:BBArchitecture}
\end{figure}

% % trace
% Job trace is from ANL's Blue Gene Intrepid system,
% containing totally 68,936 jobs run during January to September 2009\cite{JobTrace}.
% We extract two critical fields from this jobs trace: running time and
% number of cores user requested.
% In this section we take a window of 1,185 jobs and report their scheduling results.
% We patched 3 fields to each job's log entry: the amount of input data $data\_in$,
% the total amount of written data for checkpointings $data\_run$
% and the amount of output data $data\_out$.
% We assume $data\_run$ and $data\_out$ follows uniform distribution with
% lower boundary of 1 TiB and upper boundary of 60 TiB;
% $data\_in$ follows uniform distribution between 1 GiB and 30 Gib.
% The patches 3 fields may or may not be used in scheduling,
% depends on both the model of the jobs and the experiment scenario.
