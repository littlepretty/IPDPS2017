\section{Three-Phase Job Model}
\label{Sec:Model}

Before we give the details of our Cerberus design, 
in this section we describe our three-phase job model 
and the corresponding resource demand at each phase. 
 
\textbf{Stage-In}: After submitted to the HPC system,
         jobs need to prefetch data, such as configuration files, libraries and input data,
         from PFS to the burst buffer. We define this phase as \textit{stage-in}.
         The only resource needed by the job in this phase is the burst buffer.
         Users are encouraged to specify the amount of burst buffer required in the \textit{stage-in} phase.
         The job is ready to run when the data is prefetched into the burst buffer.
         If the input data is too large to read in as a whole, the \textit{stage-in} phase
         only tries to read in the first chunk of the data chunks which is sufficient for
         the application to start running in \textit{running} phase.
 
% \textbf{Running}: The scheduler allocates the job with the required
%          number of compute nodes and the amount of burst buffer. 
%          It then dispatches the job to the system for running.
%          This is defined as \textit{running} phase.
%          During the running phase, there could be frequent data exchange
%          between the compute nodes and the burst buffer.
%          For example, check-pointing could help to achieve high system fault tolerance.
%          There are also cases that input data is so large that it must be read in
%          chunks during job execution.

\textbf{Running}: The scheduler allocates the job with the required
         number of compute nodes and the amount of burst buffer. 
         It then dispatches the job to the system for running.
         This is defined as \textit{running} phase.
         During the running phase, there could be frequent data exchange
         between the compute nodes and the burst buffer. For exmaple,
         applications may periodically write 'checkpoints' of their execution. 
         There may also be in-situ/in-transit data analytics/visualization in large simulation applications.
         Some scientific workflows with coupled applications may also share, exchange and communicate
         large chunk of data during running phases\cite{Romanus:CORR:15}.  

\textbf{Stage-out}: When the job finishes computation and
         exits the \textit{running} phase, its output data need to be drained out
         to the external storage from the on-node memory. We define this as \textit{stage-out} phase.
         The job can release its compute nodes when the output data are staged out
         into the burst buffer at high speed. The burst buffer serves as an efficient
         I/O broker for the compute nodes and drains the output data to the external storage.


%While burst buffer has been utilized during all three phases,
%compute nodes are only required during the \textit{running} phase. 


%\subsection{Three-Phase Resource Demand Model}
% Users typically provide their resource demand for their jobs.
% This is the most important information scheduler could get from the users.
% Therefore, each job is associated with a demand vector in the form of $(c, rt)$
% where $c$ is the number of needed compute node in running phase,
% $rt$ is the requested runtime user can provide to help scheduler make decisions.
% To be burst buffer aware, demand vector should be augmented
% with fields defining user's request for burst buffer capacity.
% We consider two possible ways to do so.
% Corresponding to the typical usage cases of burst buffer,
% user may provide a \textit{burst buffer triple} in the form of $(bb\_in, bb\_run, bb\_out)$,
% where $bb\_in$ is the volume of burst buffer user predicted for staging in data files,
% $bb\_run$ is the volume of burst buffer user preferred for checkpointing during running,
% $bb\_out$ is the volume of burst buffer needed to hold the resulting output data.
% A job that providing demand vector augmented with
% burst buffer triple is called a \textit{3-phase modeled job}.
% Alternatively, a lazy user may just use the $\max\{bb\_in, bb\_run, bb\_out\}$ at every stage.
% %We do not make any assumption about $bb\_run$ and $bb\_out$
% %because it is nontrivial to predict them, both for system scheduler and application owner.
% We refer this augmentation as
% \textit{1-phase modeled jobs} thereafter.

%=========XY===============
Jobs are submitted to the system with the specified resource demand.
Each job is associated with a demand vector in the form of $(c, ert)$,
where $c$ is the number of needed compute nodes in the running phase,
$ert$ is the expected runtime.
In a burst-buffer-enabled system, the demand vector should be augmented
with fields defining user's requests on the burst buffer capacity.
We consider two possible ways that users provide the burst buffer demand.
For the users with accurate speculation about their job's burst buffer demand in each phase,
they may provide a \textit{burst buffer triple} in the form of $(bb\_in, bb\_run, bb\_out)$,
where $bb\_in$ is the volume of burst buffer in stage-in phase,
$bb\_run$ is the volume of the burst buffer in running phase, and
$bb\_out$ is the volume of the burst buffer for staging out the output data.
% A job that providing demand vector augmented with
% burst buffer triple is called a \textit{3-phase modeled job}.
We will show the benefits of providing the job burst buffer demand in the form of \textit{burst buffer triple} in Section~\ref{Sec:Experiments}.
Alternatively, a user may just use the $\max\{bb\_in, bb\_run, bb\_out\}$
as the job's burst buffer demand in the entire lifetime.
The three-phase job model does not assume any independence between three phases, which
discussed in more details in section~\ref{SubSec:Coordination}.
Table~\ref{Tab:Symbols} summarizes the meaning of symbols used in this paper.
% We refer this augmentation as
% \textit{1-phase modeled jobs} thereafter.

% The tightly coupling between processing cores and memory makes 3-phase model
% more complicated than it appears in terms of resource releasing.
% For a job at the stage-in phase, compute nodes is not allocated to it yet
% but that will not effect staging in data.
% When scheduler allocate compute nodes (coupled with memory) to a job,
% these nodes are exclusively used by this job.
% The first thing to do in running phase is fetching in data from burst buffer to memory,
% which is not available until compute nodes is allocated to job.
% We refer this sub-phase as \textit{fectch-in} phase.
% $bb\_in$ will be hold by job until \textit{fetch-in} phase finished.
% Even if the computation is done when job exit \textit{running phase},
% compute nodes cannot be released until job enters \textit{stage-out} phase 
% \textbf{and} output data are drained out to burst buffer from memory,
% referred as \textit{drain-out} phase.
% However, $bb\_run$ can be immediately reclaimed at the end of \textit{running phase}.
% Once \textit{drain-out} phase is done,
% compute nodes become available for jobs waiting to be run
% because staging output data out to I/O node is the business of burst buffer nodes,
% sometimes with the help from traditional I/O nodes.
% Burst buffer nodes used for caching output data can thus be put back for reuse
% when \textit{stage-out} phase, as well as the entire lifetime of job, concludes.



%The resource releasing for the 3-phase modeled jobs is nontrivial. 
%For a job at \textit{stage-in} phase, compute nodes is not allocated to it yet. 
%The job can stage in data from external storage to burst buffer without the involvement of compute nodes.
%When scheduler allocates compute nodes to the job, the job enters the \textit{running} phase 
%and starts to read the pre-fetched data from burst buffer into the main memory, 
%$bb\_in$ will be released after all the data are moved into main memory. 
%When the job finishes computation and exits \textit{running} phase, 
%$bb\_run$ are released while compute nodes are still hold by the job. 
%The compute nodes will be released after all the output data are moved into $bb\_out$ from main memory.
%The job at \textit{stage-out} phase only holds $bb\_out$ burst buffer. 
%Once all the output data are drained out to the external storage,  
%$bb\_out$ is released.


\begin{table}[h] 
        \renewcommand{\arraystretch}{1.3}
        \caption{Summary of Symbols}
        \label{Tab:Symbols}
        \centering
        \begin{tabular}{l|l}
                \hline
                $CN$ & number of total compute nodes \\
                $BB$ & amount of total burst buffer nodes \\
                $CN_{available}$ & number of available compute nodes \\
                $BB_{available}$ & amount of available burst buffer nodes \\
                $c_i$ & compute node demand of $job_i$ \\
                $bb\_in_i$ & burst buffer demand of $job_i$ at \textit{stage-in} phase \\
                $bb\_run_i$ & burst buffer demand of $job_i$ at \textit{running} phase \\
                $bb\_out_i$ & burst buffer demand of $job_i$ at \textit{stage-out} phase \\
                $rt_i$ & running time of $job_i$ recorded in the log trace \\
                $ert_i$ & estimated or expected running time of $job_i$ \\
                $Q_I, Q_R, Q_O$ & queues for \textit{stage-in, running, stage-out} phases \\
                $J_{Q_I}, J_{Q_R}, J_{Q_O}$ & set of jobs in corresponding queues \\
                $\mathcal{J}_{Q_I}, \mathcal{J}_{Q_R}, \mathcal{J}_{Q_O}$ & selected jobs from the corresponding queues\\
                $p_i$ & profit of $job_i \in J_{Q_R}$ \\
                $dp(\cdot)$ & memoization used in dynamic programming \\
                \hline
        \end{tabular}
\end{table}

