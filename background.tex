\section{Background and Motivation}
\label{Sec:Background}

% % %1. The appear of burst buffer
% % Burst buffer enabled IO architecture can help catch up with
% % the ever-increasing computational performance and parallelism of HPC system.
% % Burst buffer nodes debut as a rescue by utilizing various types of memory,
% % for example, non-volatile random-access memory (NVRAM) and solid state drive (SSD).
% % The driver behind is these storage technologies' decreasing cost of bandwidth.
% % In practice, it can suits up with DataWarp application I/O accelerator\cite{DataWarp}.
% % Figure \ref{Fig:BBArchitecture} illustrates one possible architecture of
% % burst buffer enabled HPC system, adopted by Trinity System\cite{TrinitySystem}.
% % %The volume of data read/write may affect the architecture model of burst buffer.
% % In Trinity, burst buffer nodes is composed of IO nodes and 2 PCIe SSD cards,
% % connected via totally 16 PCIe 3.0 interfaces.
% % Alternatively, many researchers proposed to distribute burst buffer 
% % on multiple layers of the memory hierarchy\cite{Romanus:CORR:15}.
% % For example, they may be deployed at local computer nodes, board in cabinet or IO nodes.
% % We may also use burst buffer as intermediate storage system.

%1. The appear of burst buffer
Burst buffer enabled I/O architecture can help alleviate
the ever-increasing I/O pressure on HPC systems.
It debuts as a rescue by utilizing various new storage technologies,
such as non-volatile random-access memory (NVRAM) and solid state drive (SSD).
In practice, burst buffer suits up with DataWarp I/O accelerator\cite{DataWarp}.
Figure \ref{Fig:BBArchitecture} illustrates one burst-buffer-enabled architecture,
adopted by the Trinity System\cite{TrinitySystem}.
%The volume of data read/write may affect the architecture model of burst buffer.
In Trinity, the burst buffer node is composed of one I/O node and 2 PCIe SSD cards,
connected via 16 PCIe 3.0 interfaces.
Alternatively, many researchers have proposed to distribute burst buffer 
on multiple layers of the memory hierarchy\cite{Romanus:CORR:15}.
For example, they may be deployed on the local compute nodes, the board in the cabinet or the I/O nodes.
Burst buffer may also be utilized as the intermediate storage systems.

% % %2. Use cases of burst buffer
% % Regardless of the specific implementation, burst buffer nodes essentially augment
% % the IO stack with a intermediate, productive offloading layer.
% % For example, an application's latest checkpoint can be pre-staged
% % before previous job terminates;
% % or an application can burst its checkpoint to burst buffer
% % with extremely high speed (4.4-17.8 TB/s on Trinity);
% % upon termination, application data is also able to drain off
% % asynchronously to external PFS.
% % When utilizing burst buffer in this primary scenario (\textit{checkpoint \& restart}),
% % bursty application IO operations can thus be aggregated and absorbed into burst buffers.
% % This makes it possible to shift computations that follows IO bursts to an earlier moment
% % while burst buffer takes charge of moving potentially TB-level volumes of data.
% % There are more use cases for burst buffer nodes.
% % Among them \textit{data cache} could be equally important to enhance the responsiveness
% % of applications by improving the perceived IO bandwidth\cite{TrinitySystem}.
% % For example, shared object library or read-only configuration files could be
% % cached on burst buffer nodes;
% % lists of input files specific to a group of compute nodes allocated to
% % a particular application could be loaded to burst buffer prior to execution;
% % Economical solid-state disks as a tier of burst buffer could also be used as
% % out-of-core complement to insufficient main memory\cite{Romanus:CORR:15},
% % working place for data analysis (reductions, feature extraction compression etc.)
% % and visualization\cite{TrinitySystem}.


%2. Use cases of burst buffer
Regardless of the specific deployment, burst buffer nodes essentially augment
the I/O stack with an intermediate productive offloading layer, 
which can benefit jobs in multiple ways.
As discussed in some preliminary surveys\cite{apex-workflow} ,
jobs' latest checkpoints can be pre-staged
before the previous job terminates;
jobs can burst checkpoint to burst buffer
with extremely high speed (4.4-17.8 TB/s on Trinity);
upon termination, jobs are also able to drain off output data
asynchronously to the external parallel file system (PFS).
The bursty I/O operations can thus be aggregated and absorbed into the burst buffer,
which shifts the computation that follows the I/O bursts to an earlier moment,
while burst buffer nodes take charge of moving potentially TB-level volumes of data.
There are more use cases for burst buffer nodes.
Among them, \textit{data cache} is equally important to enhance the responsiveness
of applications by improving the perceived I/O bandwidth\cite{TrinitySystem}.
Shared object library or read-only configuration files are cached into burst buffer;
the list of input files specific to a group of compute nodes allocated to
a particular job are loaded into burst buffer prior to the execution.
Economical solid-state disks as a tier of burst buffer could also be used as
out-of-core complement to insufficient main memory\cite{Romanus:CORR:15},
or working place for data analysis (reductions, feature extraction compression etc.)
and visualization\cite{TrinitySystem}.


% % %3. Motivate burst buffer aware scheduler
% % Given the critical role of burst buffer in future HPC IO system,
% % we expect user will be actively involved in requesting it for
% % better their own job's performance.
% % As a result, it is necessary, or even urgent, to systematically manage
% % the allocation of these second precious resources (secondary to compute nodes).
% % This naturally falls into the responsibility of HPC workload scheduler.
% % Unfortunately, existing schedulers
% % either have not yet been aware to burst buffer\cite{Moab} %other scheduler citation needed
% % or just provide very naive allocation policy\cite{SlurmBBGuide}.
% % This paper tries to bridge two isolated fields of HPC architecture,
% % the novel burst buffer equipped HPC IO subsystem and
% % traditional batch job queueing subsystem.
% % The bridge build upon a 3-phase model that motivated by two of the most
% % important usage cases of burst buffer:
% % application checkpoint restart and data cache/pre-fetch for stage in/out.
% % The benefit of burst buffer nodes will be more than just higher transfer
% % bandwidth, if it were intelligently allocated by scheduler.
% % For example, the first case could speedups the \textit{running phase} of
% % user's application by acutely absorb the bursty checkpoint-purpose IO request;
% % the second case reduce the application's waiting time via
% % contracted input/output stage in the execution pipeline of application series.
% % To our best knowledge, this is the first attempt to holistically schedule jobs running on novel burst buffer enabled storage architecture.


%3. Motivate burst buffer aware scheduler
Given the critical role of burst buffer in future HPC systems,
users are encouraged to explicitly request this new resource for performance enhancement\cite{apex-workflow}.
Inspired by this requirement, it is necessary to holistically manage
the allocation of these precious I/O resources,
which naturally falls into the responsibilities of the batch scheduler.
Some existing schedulers deals with burst buffer\cite{Moab, Cobalt} in a naive way.
For example, SLRUM simply uses straightforward allocation policy,
such as first-come first-serve\cite{SlurmBBGuide}.
This paper aims to bridge the two isolated fields of HPC architecture:
the novel burst buffer equipped HPC I/O subsystem and the
traditional batch job queueing subsystem.
The bridge builds upon a three-phase job model motivated by critical use cases of burst buffer,
i.e., application's bursty I/O pattern and data pre-fetch/cache.

In this work, we demonstrate that when burst buffer is intelligently allocated 
by our novel batch scheduler, the benefit is far beyond the higher hardware transfer bandwidth.
The job execution is expedited by having their bursty I/O requests absorbed by burst buffer.
The job waiting time is reduced via the contracted input/output stage in the scheduling pipeline.
To our best knowledge, this is the first attempt to holistically schedule jobs
running on the new burst-buffer-enabled HPC systems.

%4. Contribution summary and paper structure
%Our contributions in this paper are summarized as follows:
%\begin{enumerate}
%        \item Explore how HPC workload scheduler allocates burst buffer resources.
%                We propose a 3-phase application model tailored the typical
%                usage scenarios of burst buffer, that is, checkpoint restart,
%                data file/cache stage in and stage out.
%        \item On the basis of 3-phase job model, we present Cerberus,
%                a burst buffer aware HPC workload scheduler.
%                Dividing the lifetime of user application to different phases,
%                Cerberus makes it possible to conquer the scheduling goal separately.
%        \item We suggest several optimizing goals for each phases.
%                Though optimal scheduling problem in each phase is NP-hard,
%                dynamic programming with memorization could give precise solutions
%                in practice.
%\end{enumerate}

%In the reminder of this paper,
%the next section begins elaborating the 3-phase model (section~\ref{Sec:Model}),
%after which Cerberus is introduced in section~\ref{Sec:Scheduler}.
%The details of formulating and solving scheduling problems with
%dynamic programming at each job phase are also
%enumerated in section~\ref{Sec:Scheduler}.
%Starting from section~\ref{Sec:Experiments}, we validate Cerberus
%by simulating the full Trinity supercomputing platform, featured with
%burst buffer hardware.
%Related works are discussed in section~\ref{Sec:RelatedWorks}.
%We conclude this paper and list possible future works in section~\ref{Sec:Conclusion}.

